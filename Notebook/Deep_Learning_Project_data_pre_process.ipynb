{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt\n",
        "!pip install opencv-python-headless"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Loy-4TquE5Fz",
        "outputId": "217d4a64-610c-4891-ae1b-846444dc56b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 16512, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 16512 (delta 41), reused 49 (delta 15), pack-reused 16408\u001b[K\n",
            "Receiving objects: 100% (16512/16512), 15.17 MiB | 10.43 MiB/s, done.\n",
            "Resolving deltas: 100% (11301/11301), done.\n",
            "/content/yolov5\n",
            "Collecting gitpython>=3.1.30 (from -r requirements.txt (line 5))\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.25.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.8.0.76)\n",
            "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (9.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.11.4)\n",
            "Collecting thop>=0.1.1 (from -r requirements.txt (line 14))\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.16.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (4.66.2)\n",
            "Collecting ultralytics>=8.0.232 (from -r requirements.txt (line 18))\n",
            "  Downloading ultralytics-8.1.24-py3-none-any.whl (719 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.5/719.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (0.13.1)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (67.7.2)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30->-r requirements.txt (line 5))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2.1.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.0.232->-r requirements.txt (line 18)) (9.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2023.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n",
            "Installing collected packages: smmap, gitdb, thop, gitpython, ultralytics\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.42 smmap-5.0.1 thop-0.1.1.post2209072238 ultralytics-8.1.24\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.9.0.80)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data from Google Drive to local Colab."
      ],
      "metadata": {
        "id": "cHRsE9BaV5AA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzXoU8AfEFvm",
        "outputId": "dfa547aa-b64a-482a-914a-069cc9360bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Replace 'path_to_your_zip_file' with the path to your zip file in Google Drive\n",
        "zip_path = '/content/drive/MyDrive/Machine Learning Project/training.zip'\n",
        "\n",
        "# Replace 'destination_folder' with the path where you want to unzip your files\n",
        "destination_folder = '/content/Dataset'\n",
        "\n",
        "# Create destination directory if it does not exist\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination_folder)\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define paths\n",
        "dataset_dir = '/content/Dataset'\n",
        "images_dir = os.path.join(dataset_dir, 'images')\n",
        "labels_dir = os.path.join(dataset_dir, 'labels')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing Summary\n",
        "\n",
        "In our object detection model's development, we applied several data preprocessing techniques to enhance performance and ensure robustness:\n",
        "\n",
        "- **Grayscale to RGB Conversion**: Standardizing all images to RGB format to ensure uniform input.\n",
        "- **RGBA to RGB Conversion**: Transforming RGBA images to RGB to eliminate transparency channels, aligning image formats.\n",
        "- **Image Standardization**: Converting images to `uint8` format, aligning with augmentation library requirements.\n",
        "- **HSV Adjustments**: Randomly altering hue, saturation, and value to mimic different lighting and color settings, improving model generalization.\n",
        "- **Brightness and Contrast**: Randomly adjusting image brightness and contrast to train the model under varied lighting conditions, enhancing adaptability.\n"
      ],
      "metadata": {
        "id": "TgzPl4zDWILo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import exposure, io\n",
        "import numpy as np\n",
        "from skimage import img_as_ubyte\n",
        "import shutil\n",
        "from albumentations import Compose, HueSaturationValue, RandomBrightnessContrast\n",
        "from skimage.color import rgba2rgb, gray2rgb\n",
        "\n",
        "\n",
        "# Get a list of all image files\n",
        "all_images = os.listdir(images_dir)\n",
        "\n",
        "# Split the dataset into training and validation\n",
        "train_images, val_images = train_test_split(all_images, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "def augment_and_normalize_image(image):\n",
        "    # Check if the image is grayscale or RGBA, and convert to RGB if necessary\n",
        "    if image.ndim == 2:  # Grayscale\n",
        "        image = gray2rgb(image)\n",
        "    elif image.ndim == 3 and image.shape[2] == 4:  # RGBA to RGB\n",
        "        image = rgba2rgb(image)\n",
        "\n",
        "    # Ensure the image is uint8 before applying Albumentations augmentations\n",
        "    image = img_as_ubyte(image)\n",
        "\n",
        "    # Define augmentation pipeline\n",
        "    transform = Compose([\n",
        "        HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
        "        RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5)\n",
        "    ])\n",
        "\n",
        "    # Apply transformations\n",
        "    augmented = transform(image=image)\n",
        "    image = augmented['image']\n",
        "\n",
        "    return image\n",
        "\n",
        "def process_and_move_files(files, source_folder, dest_folder, augment=False):\n",
        "    for file in files:\n",
        "        # Process image\n",
        "        image_path = os.path.join(source_folder, file)\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        # Apply augmentations if specified\n",
        "        if augment:\n",
        "            image = augment_and_normalize_image(image)\n",
        "\n",
        "        # Save the processed image\n",
        "        io.imsave(os.path.join(dest_folder, file), image)  # Save image to destination folder\n",
        "\n",
        "        # Move corresponding label file\n",
        "        label_file = file.replace('jpg', 'txt').replace('png', 'txt')\n",
        "        shutil.move(os.path.join(source_folder.replace('images', 'labels'), label_file), dest_folder.replace('images', 'labels'))\n",
        "\n",
        "\n",
        "\n",
        "# Process and move the files\n",
        "train_dir = os.path.join(dataset_dir, 'images/train')\n",
        "val_dir = os.path.join(dataset_dir, 'images/val')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "os.makedirs(train_dir.replace('images', 'labels'), exist_ok=True)\n",
        "os.makedirs(val_dir.replace('images', 'labels'), exist_ok=True)\n",
        "\n",
        "process_and_move_files(train_images, images_dir, train_dir, augment=True)  # Apply augmentation to training images\n",
        "process_and_move_files(val_images, images_dir, val_dir)\n"
      ],
      "metadata": {
        "id": "jDSYieBFXq2l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train YOLOv5 on custom dataset for a certain number of epochs\n",
        "!python train.py --img 640 --batch 16 --epochs 50 --data /content/Dataset/dataset.yaml --weights yolov5s.pt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_E7NvwlE_3S",
        "outputId": "b3220b53-31ef-4768-a864-3c7dd46cf235"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-07 13:06:20.975829: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-07 13:06:20.975890: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-07 13:06:20.977190: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/Dataset/dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-290-gb2ffe055 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 38.3MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 277MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     21576  models.yolo.Detect                      [3, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7027720 parameters, 7027720 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Dataset/labels/train... 2144 images, 0 backgrounds, 0 corrupt: 100% 2144/2144 [00:00<00:00, 4830.20it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/Dataset/labels/train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Dataset/labels/val... 537 images, 0 backgrounds, 0 corrupt: 100% 537/537 [00:00<00:00, 1438.25it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Dataset/labels/val.cache\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.26 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/49      3.67G    0.09545     0.2137    0.03029        996        640: 100% 134/134 [01:25<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:18<00:00,  1.06s/it]\n",
            "                   all        537      14635      0.637      0.354      0.358      0.138\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/49      5.36G    0.06592     0.2052    0.01747        764        640: 100% 134/134 [01:10<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:12<00:00,  1.34it/s]\n",
            "                   all        537      14635      0.447      0.656      0.534      0.251\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/49      5.36G    0.06019     0.1979    0.01404        809        640: 100% 134/134 [01:08<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:15<00:00,  1.12it/s]\n",
            "                   all        537      14635      0.449      0.624      0.571      0.271\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/49      5.36G    0.05305     0.1946    0.01191        696        640: 100% 134/134 [01:10<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:13<00:00,  1.23it/s]\n",
            "                   all        537      14635       0.71      0.748       0.77      0.443\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/49      5.36G    0.04779     0.1916    0.01075        757        640: 100% 134/134 [01:10<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.19it/s]\n",
            "                   all        537      14635       0.81      0.775      0.824      0.475\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/49      5.36G    0.04493     0.1877    0.01033        664        640: 100% 134/134 [01:10<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.21it/s]\n",
            "                   all        537      14635      0.839      0.799      0.842      0.483\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/49      5.36G    0.04139     0.1858    0.01003        913        640: 100% 134/134 [01:09<00:00,  1.93it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.14it/s]\n",
            "                   all        537      14635      0.844      0.813      0.857      0.517\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/49      5.36G    0.03991     0.1812   0.009798        660        640: 100% 134/134 [01:11<00:00,  1.87it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:12<00:00,  1.34it/s]\n",
            "                   all        537      14635      0.859      0.803      0.861      0.523\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/49      5.36G    0.03979     0.1798    0.00965        750        640: 100% 134/134 [01:12<00:00,  1.85it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:12<00:00,  1.38it/s]\n",
            "                   all        537      14635      0.861      0.812      0.867      0.526\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/49      5.36G    0.03802     0.1824   0.009416        772        640: 100% 134/134 [01:10<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:13<00:00,  1.25it/s]\n",
            "                   all        537      14635       0.86      0.814      0.873      0.545\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/49      5.36G    0.03727     0.1795   0.009564        833        640: 100% 134/134 [01:10<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:13<00:00,  1.25it/s]\n",
            "                   all        537      14635       0.87      0.823       0.88      0.578\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      11/49      5.36G    0.03683     0.1748   0.009277        952        640: 100% 134/134 [01:11<00:00,  1.86it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:12<00:00,  1.33it/s]\n",
            "                   all        537      14635      0.869       0.82      0.877      0.573\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      12/49      5.36G    0.03561     0.1766   0.009041        689        640: 100% 134/134 [01:13<00:00,  1.81it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:12<00:00,  1.37it/s]\n",
            "                   all        537      14635      0.867       0.81      0.874      0.569\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      13/49      5.36G    0.03541     0.1711    0.00899        759        640: 100% 134/134 [01:12<00:00,  1.85it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:12<00:00,  1.37it/s]\n",
            "                   all        537      14635      0.877      0.818      0.883       0.59\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      14/49      5.36G    0.03477     0.1736   0.008872        778        640: 100% 134/134 [01:10<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.19it/s]\n",
            "                   all        537      14635      0.867      0.822      0.884      0.593\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      15/49      5.36G    0.03377     0.1714   0.008963        568        640: 100% 134/134 [01:10<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.19it/s]\n",
            "                   all        537      14635       0.87      0.829      0.889      0.605\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      16/49      5.36G    0.03338     0.1713   0.008579        853        640: 100% 134/134 [01:12<00:00,  1.86it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:13<00:00,  1.24it/s]\n",
            "                   all        537      14635      0.863      0.825      0.881      0.592\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      17/49      5.36G    0.03316     0.1685   0.008549        744        640: 100% 134/134 [01:10<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.15it/s]\n",
            "                   all        537      14635       0.87      0.823      0.885      0.606\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      18/49      5.36G    0.03339     0.1694    0.00842        723        640: 100% 134/134 [01:11<00:00,  1.87it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:13<00:00,  1.27it/s]\n",
            "                   all        537      14635      0.871      0.829      0.889      0.605\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      19/49      5.36G     0.0322     0.1679   0.008311        805        640: 100% 134/134 [01:10<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.16it/s]\n",
            "                   all        537      14635      0.831      0.766      0.834      0.542\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      20/49      5.36G    0.03233     0.1657   0.008274        564        640: 100% 134/134 [01:09<00:00,  1.93it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:13<00:00,  1.26it/s]\n",
            "                   all        537      14635      0.875      0.824      0.893      0.612\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      21/49      5.36G    0.03244     0.1666   0.008132        794        640: 100% 134/134 [01:10<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:13<00:00,  1.23it/s]\n",
            "                   all        537      14635      0.874      0.831      0.894      0.618\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      22/49      5.36G    0.03173     0.1665    0.00828        691        640: 100% 134/134 [01:12<00:00,  1.86it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:13<00:00,  1.30it/s]\n",
            "                   all        537      14635      0.873      0.827      0.894      0.595\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      23/49      5.36G    0.03174     0.1681   0.007968        809        640: 100% 134/134 [01:10<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:15<00:00,  1.10it/s]\n",
            "                   all        537      14635      0.866      0.828      0.892      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      24/49      5.36G    0.03139     0.1636   0.008106        828        640: 100% 134/134 [01:12<00:00,  1.84it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:12<00:00,  1.35it/s]\n",
            "                   all        537      14635      0.873      0.823      0.894      0.611\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      25/49      5.36G    0.03072     0.1636   0.007955        824        640: 100% 134/134 [01:10<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:13<00:00,  1.27it/s]\n",
            "                   all        537      14635       0.87      0.831      0.895      0.622\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      26/49      5.36G    0.03026     0.1614   0.007707        666        640: 100% 134/134 [01:09<00:00,  1.93it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.15it/s]\n",
            "                   all        537      14635      0.871      0.826      0.895      0.619\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      27/49      5.36G     0.0304     0.1624   0.007731        883        640: 100% 134/134 [01:10<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.18it/s]\n",
            "                   all        537      14635      0.856      0.816      0.885      0.601\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      28/49      5.36G    0.03032     0.1607   0.007591        685        640: 100% 134/134 [01:09<00:00,  1.92it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.21it/s]\n",
            "                   all        537      14635      0.875      0.828      0.895      0.609\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      29/49      5.36G    0.02988     0.1567   0.007594        702        640: 100% 134/134 [01:10<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:12<00:00,  1.34it/s]\n",
            "                   all        537      14635       0.87      0.825      0.894      0.622\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      30/49      5.36G    0.02968     0.1603    0.00756        815        640: 100% 134/134 [01:10<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.20it/s]\n",
            "                   all        537      14635      0.864       0.83      0.895      0.623\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      31/49      5.36G    0.02946     0.1593   0.007359        706        640: 100% 134/134 [01:10<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:12<00:00,  1.32it/s]\n",
            "                   all        537      14635      0.879      0.816      0.894      0.624\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      32/49      5.36G    0.02951     0.1568    0.00732        642        640: 100% 134/134 [01:09<00:00,  1.92it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.21it/s]\n",
            "                   all        537      14635      0.869      0.832      0.896      0.627\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      33/49      5.36G    0.02872     0.1548   0.006936        754        640: 100% 134/134 [01:10<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:12<00:00,  1.40it/s]\n",
            "                   all        537      14635      0.879      0.827      0.898      0.631\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      34/49      5.36G    0.02877     0.1569   0.007073        563        640: 100% 134/134 [01:11<00:00,  1.88it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:13<00:00,  1.28it/s]\n",
            "                   all        537      14635      0.874       0.84      0.901      0.635\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      35/49      5.36G    0.02825     0.1536   0.007069        896        640: 100% 134/134 [01:11<00:00,  1.88it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:12<00:00,  1.41it/s]\n",
            "                   all        537      14635      0.869      0.828      0.893      0.626\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      36/49      5.36G    0.02847     0.1543   0.006932        719        640: 100% 134/134 [01:08<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.15it/s]\n",
            "                   all        537      14635       0.88      0.828      0.897      0.629\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      37/49      5.36G    0.02842     0.1537   0.007109        861        640: 100% 134/134 [01:10<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:12<00:00,  1.31it/s]\n",
            "                   all        537      14635      0.873      0.832      0.899      0.633\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      38/49      5.36G    0.02827     0.1523   0.006741        860        640: 100% 134/134 [01:10<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:12<00:00,  1.32it/s]\n",
            "                   all        537      14635      0.849      0.826      0.887      0.624\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      39/49      5.36G    0.02794     0.1514   0.006781        588        640: 100% 134/134 [01:11<00:00,  1.88it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:13<00:00,  1.30it/s]\n",
            "                   all        537      14635      0.869      0.832      0.892      0.624\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      40/49      5.36G    0.02795     0.1513   0.006836        695        640: 100% 134/134 [01:10<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:13<00:00,  1.28it/s]\n",
            "                   all        537      14635      0.879      0.829      0.897      0.637\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      41/49      5.36G    0.02786     0.1496    0.00677        649        640: 100% 134/134 [01:10<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:13<00:00,  1.24it/s]\n",
            "                   all        537      14635      0.871      0.814      0.889      0.623\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      42/49      5.36G    0.02751     0.1511   0.006551        792        640: 100% 134/134 [01:09<00:00,  1.93it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.20it/s]\n",
            "                   all        537      14635       0.88      0.833      0.898      0.637\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      43/49      5.36G    0.02758     0.1487   0.006748        613        640: 100% 134/134 [01:10<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:13<00:00,  1.22it/s]\n",
            "                   all        537      14635      0.879      0.826      0.896      0.636\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      44/49      5.36G    0.02739      0.148   0.006508        711        640: 100% 134/134 [01:09<00:00,  1.92it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:13<00:00,  1.22it/s]\n",
            "                   all        537      14635       0.88      0.833      0.901      0.639\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      45/49      5.36G    0.02732     0.1471   0.006527        878        640: 100% 134/134 [01:09<00:00,  1.93it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.20it/s]\n",
            "                   all        537      14635      0.879      0.831      0.898      0.639\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      46/49      5.36G    0.02728     0.1482   0.006428        587        640: 100% 134/134 [01:09<00:00,  1.94it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:13<00:00,  1.29it/s]\n",
            "                   all        537      14635      0.863      0.827      0.892      0.625\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      47/49      5.36G    0.02703     0.1465   0.006498        822        640: 100% 134/134 [01:08<00:00,  1.94it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.21it/s]\n",
            "                   all        537      14635      0.882      0.834        0.9       0.64\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      48/49      5.36G    0.02747     0.1492   0.006486        726        640: 100% 134/134 [01:09<00:00,  1.92it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.19it/s]\n",
            "                   all        537      14635      0.882      0.839      0.901      0.643\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      49/49      5.36G    0.02681     0.1468   0.006335        589        640: 100% 134/134 [01:08<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:14<00:00,  1.18it/s]\n",
            "                   all        537      14635      0.886      0.829      0.901      0.642\n",
            "\n",
            "50 epochs completed in 1.184 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:29<00:00,  1.72s/it]\n",
            "                   all        537      14635      0.882      0.839      0.901      0.643\n",
            "                  room        537       5404      0.909      0.892      0.941      0.679\n",
            "                window        537       3733       0.88      0.803      0.881       0.53\n",
            "                  door        537       5498      0.858      0.821      0.881       0.72\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start training the model."
      ],
      "metadata": {
        "id": "ExmOKzfLWY3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python val.py --weights runs/train/exp/weights/best.pt --data /content/Dataset/dataset.yaml --img 640 --task val\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfl4Ag_jnQ9P",
        "outputId": "04ab687d-83f1-4451-95d5-c1f7fab44a74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/Dataset/dataset.yaml, weights=['runs/train/exp/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
            "YOLOv5 🚀 v7.0-290-gb2ffe055 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Dataset/labels/val.cache... 537 images, 0 backgrounds, 0 corrupt: 100% 537/537 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   6% 1/17 [00:06<01:41,  6.33s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  18% 3/17 [00:19<01:25,  6.14s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 17/17 [00:31<00:00,  1.88s/it]\n",
            "                   all        537      14635       0.88       0.76       0.82      0.586\n",
            "                  room        537       5404      0.904      0.807      0.858       0.62\n",
            "                window        537       3733      0.881      0.729      0.801      0.483\n",
            "                  door        537       5498      0.854      0.745      0.803      0.655\n",
            "Speed: 0.3ms pre-process, 15.7ms inference, 14.6ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing Impact Analysis on Model Performance\n",
        "\n",
        "## Overview\n",
        "\n",
        "This report provides a comparative analysis between the original and preprocessed datasets used in training a deep learning model for object detection. The goal is to evaluate the impact of data preprocessing on the model's performance.\n",
        "\n",
        "\n",
        "## Data Preprocessing Techniques\n",
        "\n",
        "In the development of our object detection model, specific data preprocessing techniques were employed to ensure the model's robustness and adaptability to varying input conditions. The following list outlines the techniques applied:\n",
        "\n",
        "1. **Conversion of Grayscale Images to RGB**: To maintain consistency in input data format, all grayscale images are converted to RGB format. This is crucial as the model is designed to process three-channel RGB images.\n",
        "\n",
        "2. **Conversion from RGBA to RGB**: Images in RGBA format, containing an alpha channel for transparency, are converted to standard RGB format. This standardization is important to avoid discrepancies in image formats and ensure uniform input to the model.\n",
        "\n",
        "3. **Image Standardization**: Prior to augmentation, images are standardized to the `uint8` format. This standardization is necessary to align with the expected input format of the augmentation library and maintain consistency across the dataset.\n",
        "\n",
        "4. **Hue, Saturation, Value Adjustments**: To introduce variability in the dataset and simulate different lighting conditions, the hue, saturation, and value of the images are randomly adjusted. This variability helps in enhancing the model's ability to generalize across different environmental settings.\n",
        "\n",
        "5. **Random Brightness and Contrast Adjustments**: The model's adaptability to different lighting conditions is further improved by randomly adjusting the brightness and contrast of the images. This step ensures that the model can perform well under various lighting conditions, enhancing its practical applicability.\n",
        "\n",
        "These preprocessing steps are integral to the training process, enhancing the model's performance and ensuring its effectiveness in real-world scenarios.\n",
        "\n",
        "## Performance Metrics Comparison\n",
        "\n",
        "### Overall Performance:\n",
        "\n",
        "- **Precision (P):** Increased from 0.879 to 0.882.\n",
        "- **Recall (R):** Significantly increased from 0.72 to 0.839.\n",
        "- **mAP50:** Increased from 0.781 to 0.901.\n",
        "- **mAP50-95:** Increased from 0.56 to 0.643.\n",
        "\n",
        "### Performance by Class:\n",
        "\n",
        "- **Room:**\n",
        "  - Precision: Remained constant at 0.909.\n",
        "  - Recall: Increased from 0.773 to 0.892.\n",
        "  - mAP50: Increased from 0.82 to 0.941.\n",
        "  - mAP50-95: Increased from 0.592 to 0.679.\n",
        "- **Window:**\n",
        "  - Precision: Slightly increased from 0.871 to 0.88.\n",
        "  - Recall: Significantly increased from 0.691 to 0.803.\n",
        "  - mAP50: Increased from 0.761 to 0.881.\n",
        "  - mAP50-95: Increased from 0.463 to 0.53.\n",
        "- **Door:**\n",
        "  - Precision: Remained constant at 0.858.\n",
        "  - Recall: Increased from 0.695 to 0.821.\n",
        "  - mAP50: Increased from 0.764 to 0.881.\n",
        "  - mAP50-95: Increased from 0.625 to 0.72.\n",
        "\n",
        "## Analysis\n",
        "\n",
        "### Overall Impact:\n",
        "\n",
        "After preprocessing, the model's overall performance has seen notable improvements, particularly in terms of Recall and Mean Average Precision (mAP). These improvements suggest that preprocessing helps the model generalize better and more effectively recognize different object classes.\n",
        "\n",
        "### Performance Variations by Class:\n",
        "\n",
        "- The **Room** category showed the most significant performance improvement, especially in Recall and mAP50, indicating that the preprocessed model is more accurate in detecting more rooms.\n",
        "- The **Window** and **Door** categories also showed performance improvements, especially in Recall, indicating that after preprocessing, the model has a higher coverage in detecting windows and doors.\n",
        "\n",
        "### Influencing Factors:\n",
        "\n",
        "The preprocessing steps include color space adjustments, and brightness and contrast adjustments. These improvements may have helped the model better distinguish between different object features, particularly under varying lighting and background conditions. The changes in color and contrast seem to aid in improving the model's ability to recognize different object categories.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The data preprocessing has significantly impacted the model's performance positively, especially in terms of Recall and mAP metrics. This indicates that preprocessing steps like color adjustments and brightness/contrast adjustments are effective in enhancing the model's generalization ability in real-world scenarios. The specific improvements in recognizing certain object categories, such as rooms, windows, and doors, suggest these preprocessing techniques are particularly useful in enhancing the model's ability to detect specific objects. Further experimentation, such as different types of image enhancements, could be beneficial to determine the optimal data preprocessing workflow.\n"
      ],
      "metadata": {
        "id": "Op0_427LT9oR"
      }
    }
  ]
}